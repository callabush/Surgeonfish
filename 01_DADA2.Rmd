---
title: "Infer ASVs with DADA2"
author: "Calla Bush St George"
date: "`r Sys.Date()`"
output:
  html_document: 
    code_folding: show
    theme: spacelab
    highlight: pygments
    keep_md: no
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
      toc_depth: 3
  keep_md: true  
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center",
                      fig.path = "../figures/01_DADA2/") #Send any figure output to this folder
```

## Before you start

### Set my seed
```{r set seed}
# Any number can be chose
set.seed(567890)
```

### Load Libraries

```{r load-libraries}
# Efficient package loading with pacman 
# Don't forget to install pacman and DT if you don't have it yet. :) 
pacman::p_load(tidyverse, devtools, dada2, phyloseq, patchwork, DT, 
               install = FALSE)
```



# Goals for this file

1.  Use raw fastq and generate the quality plots to asses the quality of reads

2.  Filter and trim out bad sequences and bases from our sequencing files

3.  Write out fastq files with high quality sequences

4.  Evaluate the quality from our filter and trim.

5. Infer errors on forward and reverse reads individually

6. Identified ASVs on forward and reverse reads separately using the error model.

7. Merge forward and reverse ASVs into "contigous ASVs".

8. Generate ASV count table. (`otu_table` input for phyloseq.).



# Output that we need:

1. ASV count table: `otu_table`

2. Taxonomy table `tax_table`

3. Sample information: `sample_table` track the reads lost throughout DADA2 workflow.


## Load Data

```{r load data}
# Set the raw fastq path to the raw sequencing files
# Path to the fastq files
raw_fastqs_path <- "data/01_DADA2/01_raw_gzipped_fastqs"

# What files are in this path (Intuition check)
list.files(raw_fastqs_path)

# How many files are there?
str(list.files(raw_fastqs_path))

# Create a vector of forward reads
forward_reads <- 
  list.files(raw_fastqs_path, pattern = "_raw_1.fq.gz", 
             full.names = TRUE) 

# Intuition check
head(forward_reads)

# Create a vector of reverse reads
reverse_reads <-
  list.files(raw_fastqs_path, pattern = "_raw_2.fq.gz",
                           full.names = TRUE)
# Intuition check
head(reverse_reads)

# Any duplicates between forward and reverse reads
any(duplicated(c(forward_reads, reverse_reads)))
```

# Assess Raw Read Quality 

## Evaluate raw sequence quality 

Let's see the quality of the raw reads *before* we trim

### Plot 12 random samples of plots 
```{r raw-quality-plot, fig.width=12, fig.height=8}
# Randomly select 12 samples from dataset to evaluate 
# Selecting 12 is typically better than 2 (like we did in class for efficiency)
random_samples <- sample(1:length(reverse_reads), size = 12)
random_samples

# Calculate and plot quality of these two samples
forward_filteredQual_plot_12 <- plotQualityProfile(forward_reads[random_samples]) + 
  labs(title = "Forward Read Raw Quality")

reverse_filteredQual_plot_12 <- plotQualityProfile(reverse_reads[random_samples]) + 
  labs(title = "Reverse Read Raw Quality")


# Plot them together with patchwork
forward_filteredQual_plot_12 + reverse_filteredQual_plot_12

```

### Aggregated Raw Quality Plots 
```{r raw-aggregate-plot, fig.width=5.5, fig.height=3.5, eval = FALSE, echo = TRUE}
# Aggregate all QC plots 
plotQualityProfile(forward_reads, aggregate = TRUE) + 
  plotQualityProfile(reverse_reads, aggregate = TRUE)
```

## Prepare a placeholder for filtered reads

```{r prep-filtered-reads}
# vector of our samples, extract the sample information from our file
samples <- sapply(strsplit(basename(forward_reads), "_"), function(x)paste(x[1],x[2],sep="_"))
#Intuition check
head(samples)

#place filtered reads into filtered_fastqs_path
filtered_fastqs_path <- "data/01_DADA2/02_filtered_fastqs"
filtered_fastqs_path

# create 2 variables : filtered_F, filtered_R
filtered_forward_reads <- file.path(filtered_fastqs_path, paste0(samples, "_R1_filtered.fastq.gz"))

#Intuition check
head(filtered_forward_reads)
length(filtered_forward_reads)

filtered_reverse_reads <- file.path(filtered_fastqs_path, paste0(samples, "_R2_filtered.fastq.gz"))

#Intuition check
head(filtered_reverse_reads)
length(filtered_reverse_reads)

```

# Filter and Trim Reads

Parameters of filter and trim **DEPEND ON THE DATASET**

- `maxN` = number of N bases. Remove all Ns from the data. 
- `maxEE` = quality filtering threshold applied to expected errors. By default, all expected errors. Mar recommends using c(1,1). Here, if there is maxEE expected errors, its okay. If more, throw away sequence.
- `trimLeft` = trim certain number of base pairs on start of each read
- `truncQ` = truncate reads at the first instance of a quality score less than or equal to selected number. Chose 2 
- `rm.phix` = remove phi x
- `compress` = make filtered files .gzipped
- `multithread` = multithread

```{r filter-and-trim}
#Assign a vector to filtered reads
#Trim out poor bases, none in this instance.
#Write out filtered fastq files
filtered_reads <-
  filterAndTrim(fwd = forward_reads, filt = filtered_forward_reads,
              rev = reverse_reads, filt.rev = filtered_reverse_reads,
              trimLeft = c(17,21),truncLen = c(225, 216),
              maxN = 0, maxEE = c(1, 1),truncQ = 2, rm.phix = TRUE,
              compress = TRUE, multithread = 6)

# Describes library prep
#Forward and reverse read have slight overlap, V3-V4 region is sequenced
# 341F and 806R

```

# Assess trimmed read quality

```{r filterTrim-quality-plots,  fig.width=12, fig.height=8}
# Plot the 12 random samples after QC
forward_filteredQual_plot_12 <- 
  plotQualityProfile(filtered_forward_reads[random_samples]) + 
  labs(title = "Trimmed Forward Read Quality")

reverse_filteredQual_plot_12 <- 
  plotQualityProfile(filtered_reverse_reads[random_samples]) + 
  labs(title = "Trimmed Reverse Read Quality")

# Put the two plots together 
forward_filteredQual_plot_12 + reverse_filteredQual_plot_12
```

## Aggregated Trimmed Plots
```{r qc-aggregate-plot, fig.width=5.5, fig.height=3.5, eval = FALSE, echo = TRUE}
#Aggregate all QC plots
plotQualityProfile(filtered_forward_reads, aggregate = TRUE) + 
 plotQualityProfile(filtered_reverse_reads, aggregate = TRUE)
```

## Stats on read output from `filterAndTrim`

```{r filterTrim-stats}
#Make output into dataframe
filtered_df <- as.data.frame(filtered_reads)
head(filtered_df)

# calculate some stats
filtered_df %>%
  reframe(median_reads_in = median(reads.in),
          median_reads_out = median(reads.out),
          median_percent_retained = (median(reads.out)/median(reads.in)))

```

# Error Modelling 

**Note every sequencing run needs to be run separately!** The error model *MUST* be run separately on each illumina dataset. If you'd like to combine the datasets from multiple sequencing runs, you'll need to do the exact same `filterAndTrim()` step *AND*, very importantly, you'll need to have the same primer and ASV length expected by the output.

The dada2 pipeline is written for data that has HiSeq or MiSeq quality scores (ranging from 0-40). NovaSeq uses “binned” quality scores (instead of 0 and 40, there are only 4 scores). Typically, the Novaseq sequencer converts the 0-40 scores as shown below:

0-2 → 2 3-14 → 11 15-30 → 25 31-40 → 37

Error plots generated look strange for Novaseq data because of the binned quality scores. Eventually, the dada2 pipeline will likely have a mode for Novaseq data, but this has not been rolled out yet, so for now the solution is some workaround code that enforces monotonicity as indicated below. See the discussion: https://github.com/benjjneb/dada2/issues/791. Other possible solutions (including altering the loess weights) have been suggested in another discussion: https://github.com/benjjneb/dada2/issues/1307.

The Ernakovich Lab has a suggested pipeline that tests four possible error models alterations: https://github.com/ErnakovichLab/dada2_ernakovichlab/tree/split_for_premise. These options will be tested on this data.
```{r learn-errors, fig.width=12, fig.height=8}
#Forward reads
errF <-
  learnErrors(filtered_forward_reads, multithread = 6)

#Reverse reads
errR <-
  learnErrors(filtered_reverse_reads, multithread = 6)
```

## Option 4: alter loess function arguments (weights and span and degree, also enforce monotonicity)
This is based on the Ernakovich Lab pipeline, but uses nbases = 1e8 to reduce running time by sampling less reads.

```{r learn-errors-option-4, fig.width=12, fig.height=8}
loessErrfun_mod4 <- function(trans) {
  qq <- as.numeric(colnames(trans))
  est <- matrix(0, nrow=0, ncol=length(qq))
  for(nti in c("A","C","G","T")) {
    for(ntj in c("A","C","G","T")) {
      if(nti != ntj) {
        errs <- trans[paste0(nti,"2",ntj),]
        tot <- colSums(trans[paste0(nti,"2",c("A","C","G","T")),])
        rlogp <- log10((errs+1)/tot)  # 1 psuedocount for each err, but if tot=0 will give NA
        rlogp[is.infinite(rlogp)] <- NA
        df <- data.frame(q=qq, errs=errs, tot=tot, rlogp=rlogp)
        
        # jonalim's solution
        # https://github.com/benjjneb/dada2/issues/938
        mod.lo <- loess(rlogp ~ q, df, weights = log10(tot),degree = 1, span = 0.95)
        
        pred <- predict(mod.lo, qq)
        maxrli <- max(which(!is.na(pred)))
        minrli <- min(which(!is.na(pred)))
        pred[seq_along(pred)>maxrli] <- pred[[maxrli]]
        pred[seq_along(pred)<minrli] <- pred[[minrli]]
        est <- rbind(est, 10^pred)
      } # if(nti != ntj)
    } # for(ntj in c("A","C","G","T"))
  } # for(nti in c("A","C","G","T"))
  
  # HACKY
  MAX_ERROR_RATE <- 0.25
  MIN_ERROR_RATE <- 1e-7
  est[est>MAX_ERROR_RATE] <- MAX_ERROR_RATE
  est[est<MIN_ERROR_RATE] <- MIN_ERROR_RATE
  
  # enforce monotonicity
  # https://github.com/benjjneb/dada2/issues/791
  estorig <- est
  est <- est %>%
    data.frame() %>%
    mutate_all(funs(case_when(. < X40 ~ X40,
                              . >= X40 ~ .))) %>% as.matrix()
  rownames(est) <- rownames(estorig)
  colnames(est) <- colnames(estorig)
  
  # Expand the err matrix with the self-transition probs
  err <- rbind(1-colSums(est[1:3,]), est[1:3,],
               est[4,], 1-colSums(est[4:6,]), est[5:6,],
               est[7:8,], 1-colSums(est[7:9,]), est[9,],
               est[10:12,], 1-colSums(est[10:12,]))
  rownames(err) <- paste0(rep(c("A","C","G","T"), each=4), "2", c("A","C","G","T"))
  colnames(err) <- colnames(trans)
  # Return
  return(err)
}

# check what this looks like
errF_4 <- learnErrors(
  filtered_forward_reads,
  multithread = 6,
  nbases = 1e8,
  errorEstimationFunction = loessErrfun_mod4,
  verbose = TRUE
)

errR_4 <- learnErrors(
  filtered_reverse_reads,
  multithread = 6,
  nbases = 1e8,
  errorEstimationFunction = loessErrfun_mod4,
  verbose = TRUE
)

#plots option 4 (enforce montonicity only)
errF_plot4 <- plotErrors(errF_4, nominalQ = TRUE)
errR_plot4 <-plotErrors(errR_4, nominalQ = TRUE)

#check that values have gotten much smaller from where they started, because the "self-consistency loop terminated before convergence" error
dada2:::checkConvergence(errR_4)
dada2:::checkConvergence(errF_4)

#shows plots
errF_plot4
errR_plot4
```


### Interpreting Plots

In the error model plots, the red line shows the expected learned error rates for each of the 16 possible base transitions and transversions. The black line and grey dots shows the observed error rates. How closely the black and red lines align shows the accuracy of the error model.

For NovaSeq data, there is a characteristic dip in the error plots when using the default error model. The four options above try to overcome this issue, none of which are ideal. To select an error model from the options, examine whether the black line is decreasing (as quality scores increase the predicted error rate decreases). Also try to select a plot where the points that mostly fall near the black lines. Note that some points will probably fall on the x-axis.

Based on this data, the option 4 error model seems best. Option 1 and 4 are clearly better than options 2 and 3, and between these better options the A to C and G to A look better in option 4.

Details of the plot: 
- **Points**: The observed error rates for each consensus quality score.  
- **Black line**: Estimated error rates after convergence of the machine-learning algorithm.  
- **Red line:** The error rates expected under the nominal definition of the Q-score.  

Similar to what is mentioned in the dada2 tutorial: the estimated error rates (black line) are a "reasonably good" fit to the observed rates (points), and the error rates drop with increased quality as expected.  We can now infer ASVs! 

# Infer ASVs 

**An important note:** This process occurs separately on forward and reverse reads! This is quite a different approach from how OTUs are identified in Mothur and also from UCHIME, oligotyping, and other OTU, MED, and ASV approaches.

```{r infer-ASVs}
#Infer forward ASVs
dada_forward <- dada(filtered_forward_reads, 
                     err = errF_4, multithread = 6)
typeof(dada_forward)

#Infer reverse ASVs
dada_reverse <- dada(filtered_reverse_reads, 
                     err = errR_4, multithread = 6)

# Inspect 
dada_reverse[1]
dada_reverse[12]
```

### Sample 13 and 14 have low reads. These are our negative controls. 

# Merge Forward & Reverse ASVs

Now, merge the forward and reverse ASVs into contigs. 

```{r merge_ASVs}
# merge forward and reverse ASVs
merged_ASVs <- mergePairs(dada_forward, filtered_forward_reads, 
                          dada_reverse, filtered_reverse_reads, 
                          verbose = TRUE)

# Evaluate output
typeof(merged_ASVs)
length(merged_ASVs)
names(merged_ASVs)
```


# Create Raw ASV Count Table 
```{r generate-ASV-table, fig.width=3.5, fig.height=3}
# Create the ASV Count Table 
raw_ASV_table <- makeSequenceTable(merged_ASVs)

# Write out the file to data/01_DADA2


# Check the type and dimensions of the data
dim(raw_ASV_table)
class(raw_ASV_table)
typeof(raw_ASV_table)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table)))

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Raw distribution of ASV length")


###################################################
###################################################
# TRIM THE ASVS
# Let's trim the ASVs to only be the right size, which is 236.
# 236 originates from our expected amplicon being trimmed due to low quality at the ends

raw_ASV_table_trimmed <- raw_ASV_table[,nchar(colnames(raw_ASV_table)) %in% 414:420]

# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table_trimmed)))

# What proportion is left of the sequences? 
sum(raw_ASV_table_trimmed)/sum(raw_ASV_table)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table_trimmed))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Trimmed distribution of ASV length")

```


# Remove Chimeras

Sometimes chimeras arise in our workflow. 

**Chimeric sequences** are artificial sequences formed by the combination of two or more distinct biological sequences. These chimeric sequences can arise during the polymerase chain reaction (PCR) amplification step of the 16S rRNA gene, where fragments from different templates can be erroneously joined together.

Chimera removal is an essential step in the analysis of 16S sequencing data to improve the accuracy of downstream analyses, such as taxonomic assignment and diversity assessment. It helps to avoid the inclusion of misleading or spurious sequences that could lead to incorrect biological interpretations.

```{r rm_chimeras, fig.width=3.5, fig.height=3}
# Remove the chimeras in the raw ASV table
noChimeras_ASV_table <- removeBimeraDenovo(raw_ASV_table_trimmed, 
                                           method="consensus", 
                                           multithread=6, verbose=TRUE)

# Check the dimensions
dim(noChimeras_ASV_table)

# What proportion is left of the sequences? 
sum(noChimeras_ASV_table)/sum(raw_ASV_table_trimmed)
sum(noChimeras_ASV_table)/sum(raw_ASV_table)

# Plot it 
data.frame(Seq_Length_NoChim = nchar(getSequences(noChimeras_ASV_table))) %>%
  ggplot(aes(x = Seq_Length_NoChim )) + 
  geom_histogram()+ 
  labs(title = "Trimmed + Chimera Removal distribution of ASV length")

```



# Track the read counts
Here, we will look at the number of reads that were lost in the filtering, denoising, merging, and chimera removal. 
```{r track_reads, fig.width=6, fig.height=4}
# A little function to identify number seqs 
getN <- function(x) sum(getUniques(x))

# Make the table to track the seqs 
track <- cbind(filtered_reads, 
               sapply(dada_forward, getN),
               sapply(dada_reverse, getN),
               sapply(merged_ASVs, getN),
               rowSums(noChimeras_ASV_table))

head(track)

# Update column names to be more informative (most are missing at the moment!)
colnames(track) <- 
  c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track) <- samples

# Generate a dataframe to track the reads through our DADA2 pipeline
track_counts_df <- 
  track %>%
  # make it a dataframe
  as.data.frame() %>%
  rownames_to_column(var = "names") %>%
  mutate(perc_reads_retained = 100 * nochim / input)

# Visualize it in table format 
DT::datatable(track_counts_df)

# Plot it!
track_counts_df %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  mutate(read_type = fct_relevel(read_type, 
                                 "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")) %>%
  ggplot(aes(x = read_type, y = num_reads, fill = read_type)) + 
  geom_line(aes(group = names), color = "grey") + 
  geom_point(shape = 21, size = 3, alpha = 0.8) + 
  scale_fill_brewer(palette = "Spectral") + 
  labs(x = "Filtering Step", y = "Number of Sequences") + 
  theme_bw()
```



# Assign Taxonomy 

Here, we will use the silva database version 138!
```{r assign-tax}
taxa_train <- 
  assignTaxonomy(noChimeras_ASV_table, 
      "/Users/cab565/Documents/Surgeonfish/data/01_DADA2/03_taxonomy/silva_nr99_v138.1_train_set.fa.gz", 
                 multithread=6)

taxa_addSpecies <- 
  addSpecies(taxa_train, 
      "/Users/cab565/Documents/Surgeonfish/data/01_DADA2/03_taxonomy/silva_species_assignment_v138.1.fa.gz")

# Inspect the taxonomy 
taxa_print <- taxa_addSpecies # Removing sequence rownames for display only
rownames(taxa_print) <- NULL
#View(taxa_print)
```




# Prepare the data for export! 

## 1. ASV Table 

Below, we will prepare the following: 

1. Two ASV Count tables: 
      a. With ASV seqs: ASV headers include the *entire* ASV sequence ~250bps.
      b. with ASV names: This includes re-written and shortened headers like ASV_1, ASV_2, etc, which will match the names in our fasta file below.  
2. `ASV_fastas`: A fasta file that we can use to build a tree for phylogenetic analyses (e.g. phylogenetic alpha diversity metrics or UNIFRAC dissimilarty).  

### Finalize ASV Count Tables 
```{r prepare-ASVcount-table}
########### 2. COUNT TABLE ###############
############## Modify the ASV names and then save a fasta file!  ############## 
# Give headers more manageable names
# First pull the ASV sequences
asv_seqs <- colnames(noChimeras_ASV_table)
asv_seqs[1:5]

# make headers for our ASV seq fasta file, which will be our asv names
asv_headers <- vector(dim(noChimeras_ASV_table)[2], mode = "character")
asv_headers[1:5]

# loop through vector and fill it in with ASV names 
for (i in 1:dim(noChimeras_ASV_table)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep = "_")
}

# intitution check
asv_headers[1:5]

##### Rename ASVs in table then write out our ASV fasta file! 
#View(noChimeras_ASV_table)
asv_tab <- t(noChimeras_ASV_table)
#View(asv_tab)

## Rename our asvs! 
row.names(asv_tab) <- sub(">", "", asv_headers)
#View(asv_tab)
```


## 2. Taxonomy Table 
```{r prepare-tax-table}
# Inspect the taxonomy table
#View(taxa_addSpecies)

##### Prepare tax table 
# Add the ASV sequences from the rownames to a column 
new_tax_tab <- 
  taxa_addSpecies%>%
  as.data.frame() %>%
  rownames_to_column(var = "ASVseqs") 
head(new_tax_tab)

# intution check 
stopifnot(new_tax_tab$ASVseqs == colnames(noChimeras_ASV_table))

# Now let's add the ASV names 
rownames(new_tax_tab) <- rownames(asv_tab)
head(new_tax_tab)

### Final prep of tax table. Add new column with ASV names 
asv_tax <- 
  new_tax_tab %>%
  # add rownames from count table for phyloseq handoff
  mutate(ASV = rownames(asv_tab)) %>%
  # Resort the columns with select
  dplyr::select(Kingdom, Phylum, Class, Order, Family, Genus, Species, ASV, ASVseqs)

head(asv_tax)

# Intution check
stopifnot(asv_tax$ASV == rownames(asv_tax), rownames(asv_tax) == rownames(asv_tab))
```



# Write `01_DADA2` files

Now, we will write the files! We will write the following to the `data/01_DADA2/` folder. We will save both as files that could be submitted as supplements AND as .RData objects for easy loading into the next steps into R.:  

1. `ASV_counts.tsv`: ASV count table that has ASV names that are re-written and shortened headers like ASV_1, ASV_2, etc, which will match the names in our fasta file below. This will also be saved as `data/01_DADA2/ASV_counts.RData`.
2. `ASV_counts_withSeqNames.tsv`: This is generated with the data object in this file known as `noChimeras_ASV_table`. ASV headers include the *entire* ASV sequence ~250bps.  In addition, we will save this as a .RData object as `data/01_DADA2/noChimeras_ASV_table.RData` as we will use this data in `analysis/02_Taxonomic_Assignment.Rmd` to assign the taxonomy from the sequence headers.  
3. `ASVs.fasta`: A fasta file output of the ASV names from `ASV_counts.tsv` and the sequences from the ASVs in `ASV_counts_withSeqNames.tsv`. A fasta file that we can use to build a tree for phylogenetic analyses (e.g. phylogenetic alpha diversity metrics or UNIFRAC dissimilarty).  
4. We will also make a copy of `ASVs.fasta` in `data/02_TaxAss_FreshTrain/` to be used for the taxonomy classification in the next step in the workflow.  
5. Write out the taxonomy table
6. `track_read_counts.RData`: To track how many reads we lost throughout our workflow that could be used and plotted later. We will add this to the metadata in `analysis/02_Taxonomic_Assignment.Rmd`.   


```{r save-files}
# FIRST, we will save our output as regular files, which will be useful later on. 
# Save to regular .tsv file 
# Write BOTH the modified and unmodified ASV tables to a file!
# Write count table with ASV numbered names (e.g. ASV_1, ASV_2, etc)
write.table(asv_tab, "data/01_DADA2/ASV_counts.tsv", sep = "\t", quote = FALSE, col.names = NA)
# Write count table with ASV sequence names
write.table(noChimeras_ASV_table, "data/01_DADA2/ASV_counts_withSeqNames.tsv", sep = "\t", quote = FALSE, col.names = NA)
# Write out the fasta file for reference later on for what seq matches what ASV
asv_fasta <- c(rbind(asv_headers, asv_seqs))
# Save to a file!
write(asv_fasta, "data/01_DADA2/ASVs.fasta")


# SECOND, let's save the taxonomy tables 
# Write the table 
write.table(asv_tax, "data/01_DADA2/ASV_taxonomy.tsv", sep = "\t", quote = FALSE, col.names = NA)


# THIRD, let's save to a RData object 
# Each of these files will be used in the analysis/02_Taxonomic_Assignment
# RData objects are for easy loading :) 
save(noChimeras_ASV_table, file = "data/01_DADA2/noChimeras_ASV_table.RData")
save(asv_tab, file = "data/01_DADA2/ASV_counts.RData")
# And save the track_counts_df a R object, which we will merge with metadata information in the next step of the analysis in nalysis/02_Taxonomic_Assignment. 
save(track_counts_df, file = "data/01_DADA2/track_read_counts.RData")
```


```{r asv-tab}
write.csv2(asv_tab, file = "data/01_DADA2/ASV_counts.csv")
```



##Session information
```{r session-info}
#Ensure reproducibility
devtools::session_info()
```



